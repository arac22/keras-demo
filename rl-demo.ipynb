{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Code\n\nhttps://github.com/nicknochnack/OpenAI-Reinforcement-Learning-with-Custom-Environment\n\nVideo\n\nhttps://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbk1Iang2ME0wODgzTlctRVl6LUdBRFU0WFloZ3xBQ3Jtc0tua2k5cFpNbzlNX2UxWmlzXzl5MzlZX3VPd0RoZ1Y3UVpEUjl5MWkweWs4TFM2MmpYWUdESEI0VmU2TndCVkJUaWFUUmZraVFGN3p5WWdyalRzOFhxcmdfU2tRSkwzdGlyMDN3eGFVRVh5RWhQWklicw&q=https%3A%2F%2Fgithub.com%2Fnicknochnack%2FOpenAI-Reinforcement-Learning-with-Custom-Environment&v=bD6V3rcr_54\n\nSolution to compatibility problems\n\nhttps://stackoverflow.com/questions/59765784/attributeerror-sequential-object-has-no-attribute-get-distribution-strategy\n","metadata":{}},{"cell_type":"markdown","source":"# 0. Install Dependencies","metadata":{}},{"cell_type":"code","source":"# !pip install tensorflow\n#!pip install gym\n#!pip install keras\n# !pip install keras-rl2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Test Random Environment with OpenAI Gym","metadata":{}},{"cell_type":"code","source":"from gym import Env\nfrom gym.spaces import Discrete, Box\nimport numpy as np\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:45:31.873613Z","iopub.execute_input":"2022-10-06T16:45:31.874086Z","iopub.status.idle":"2022-10-06T16:45:31.880940Z","shell.execute_reply.started":"2022-10-06T16:45:31.874050Z","shell.execute_reply":"2022-10-06T16:45:31.879226Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"class ShowerEnv(Env):\n    def __init__(self):\n        # Actions we can take, down, stay, up\n        self.action_space = Discrete(3)\n        # Temperature array\n        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n        # Set start temp\n        self.state = 38 + random.randint(-3,3)\n        # Set shower length\n        self.shower_length = 60\n        \n    def step(self, action):\n        # Apply action\n        # 0 -1 = -1 temperature\n        # 1 -1 = 0 \n        # 2 -1 = 1 temperature \n        self.state += action -1 \n        # Reduce shower length by 1 second\n        self.shower_length -= 1 \n        \n        # Calculate reward\n        if self.state >=37 and self.state <=39: \n            reward =1 \n        else: \n            reward = -1 \n        \n        # Check if shower is done\n        if self.shower_length <= 0: \n            done = True\n        else:\n            done = False\n        \n        # Apply temperature noise\n        #self.state += random.randint(-1,1)\n        # Set placeholder for info\n        info = {}\n        \n        # Return step information\n        return self.state, reward, done, info\n\n    def render(self):\n        # Implement viz\n        pass\n    \n    def reset(self):\n        # Reset shower temperature\n        self.state = 38 + random.randint(-3,3)\n        # Reset shower time\n        self.shower_length = 60 \n        return self.state\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:05.793028Z","iopub.execute_input":"2022-10-06T16:46:05.793956Z","iopub.status.idle":"2022-10-06T16:46:05.807790Z","shell.execute_reply.started":"2022-10-06T16:46:05.793915Z","shell.execute_reply":"2022-10-06T16:46:05.806468Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"env = ShowerEnv()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:08.234336Z","iopub.execute_input":"2022-10-06T16:46:08.234813Z","iopub.status.idle":"2022-10-06T16:46:08.241172Z","shell.execute_reply.started":"2022-10-06T16:46:08.234776Z","shell.execute_reply":"2022-10-06T16:46:08.239883Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"env.observation_space.sample()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:09.973409Z","iopub.execute_input":"2022-10-06T16:46:09.973936Z","iopub.status.idle":"2022-10-06T16:46:09.986003Z","shell.execute_reply.started":"2022-10-06T16:46:09.973896Z","shell.execute_reply":"2022-10-06T16:46:09.984469Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"array([30.645672], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"episodes = 10\nfor episode in range(1, episodes+1):\n    state = env.reset()\n    done = False\n    score = 0 \n    \n    while not done:\n        # env.render(mode=\"ipython\")\n        action = env.action_space.sample()\n        n_state, reward, done, info = env.step(action)\n        score+=reward\n    print('Episode:{} Score:{}'.format(episode, score))","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:11.635151Z","iopub.execute_input":"2022-10-06T16:46:11.635674Z","iopub.status.idle":"2022-10-06T16:46:11.649492Z","shell.execute_reply.started":"2022-10-06T16:46:11.635611Z","shell.execute_reply":"2022-10-06T16:46:11.647985Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"Episode:1 Score:-46\nEpisode:2 Score:22\nEpisode:3 Score:-10\nEpisode:4 Score:-48\nEpisode:5 Score:-32\nEpisode:6 Score:-44\nEpisode:7 Score:8\nEpisode:8 Score:-32\nEpisode:9 Score:-50\nEpisode:10 Score:-6\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Create a Deep Learning Model with Keras","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras as keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten\nfrom keras.optimizers import adam_v2","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:14.646372Z","iopub.execute_input":"2022-10-06T16:46:14.647222Z","iopub.status.idle":"2022-10-06T16:46:14.653312Z","shell.execute_reply.started":"2022-10-06T16:46:14.647177Z","shell.execute_reply":"2022-10-06T16:46:14.651995Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"states = env.observation_space.shape\nactions = env.action_space.n","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:16.179992Z","iopub.execute_input":"2022-10-06T16:46:16.180401Z","iopub.status.idle":"2022-10-06T16:46:16.185822Z","shell.execute_reply.started":"2022-10-06T16:46:16.180369Z","shell.execute_reply":"2022-10-06T16:46:16.184749Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"states","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:17.818997Z","iopub.execute_input":"2022-10-06T16:46:17.819493Z","iopub.status.idle":"2022-10-06T16:46:17.827428Z","shell.execute_reply.started":"2022-10-06T16:46:17.819454Z","shell.execute_reply":"2022-10-06T16:46:17.826083Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"(1,)"},"metadata":{}}]},{"cell_type":"code","source":"def build_model(states, actions):\n    model = Sequential()    \n    model.add(Dense(24, activation='relu', input_shape=states))\n    model.add(Dense(24, activation='relu'))\n    model.add(Dense(actions, activation='linear'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:19.835453Z","iopub.execute_input":"2022-10-06T16:46:19.836710Z","iopub.status.idle":"2022-10-06T16:46:19.843593Z","shell.execute_reply.started":"2022-10-06T16:46:19.836630Z","shell.execute_reply":"2022-10-06T16:46:19.842068Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# del model ","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:21.476809Z","iopub.execute_input":"2022-10-06T16:46:21.477191Z","iopub.status.idle":"2022-10-06T16:46:21.482459Z","shell.execute_reply.started":"2022-10-06T16:46:21.477162Z","shell.execute_reply":"2022-10-06T16:46:21.481232Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"model = build_model(states, actions)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:23.045048Z","iopub.execute_input":"2022-10-06T16:46:23.046489Z","iopub.status.idle":"2022-10-06T16:46:23.102402Z","shell.execute_reply.started":"2022-10-06T16:46:23.046433Z","shell.execute_reply":"2022-10-06T16:46:23.101069Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:25.407290Z","iopub.execute_input":"2022-10-06T16:46:25.407762Z","iopub.status.idle":"2022-10-06T16:46:25.415805Z","shell.execute_reply.started":"2022-10-06T16:46:25.407725Z","shell.execute_reply":"2022-10-06T16:46:25.414327Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Model: \"sequential_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_22 (Dense)             (None, 24)                48        \n_________________________________________________________________\ndense_23 (Dense)             (None, 24)                600       \n_________________________________________________________________\ndense_24 (Dense)             (None, 3)                 75        \n=================================================================\nTotal params: 723\nTrainable params: 723\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Build Agent with Keras-RL","metadata":{}},{"cell_type":"code","source":"from rl.agents import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:32.068906Z","iopub.execute_input":"2022-10-06T16:46:32.069337Z","iopub.status.idle":"2022-10-06T16:46:32.076897Z","shell.execute_reply.started":"2022-10-06T16:46:32.069295Z","shell.execute_reply":"2022-10-06T16:46:32.075173Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def build_agent(model, actions):\n    policy = BoltzmannQPolicy()\n    memory = SequentialMemory(limit=50000, window_length=1)\n    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n    return dqn","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:33.616089Z","iopub.execute_input":"2022-10-06T16:46:33.616598Z","iopub.status.idle":"2022-10-06T16:46:33.623440Z","shell.execute_reply.started":"2022-10-06T16:46:33.616560Z","shell.execute_reply":"2022-10-06T16:46:33.622347Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"dqn = build_agent(model, actions)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:35.299532Z","iopub.execute_input":"2022-10-06T16:46:35.299969Z","iopub.status.idle":"2022-10-06T16:46:35.305504Z","shell.execute_reply.started":"2022-10-06T16:46:35.299932Z","shell.execute_reply":"2022-10-06T16:46:35.304188Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"dqn.compile(adam_v2.Adam(learning_rate=1e-3), metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:36.700137Z","iopub.execute_input":"2022-10-06T16:46:36.700550Z","iopub.status.idle":"2022-10-06T16:46:37.291323Z","shell.execute_reply.started":"2022-10-06T16:46:36.700517Z","shell.execute_reply":"2022-10-06T16:46:37.289925Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:46:39.273206Z","iopub.execute_input":"2022-10-06T16:46:39.273715Z","iopub.status.idle":"2022-10-06T16:50:15.019817Z","shell.execute_reply.started":"2022-10-06T16:46:39.273672Z","shell.execute_reply":"2022-10-06T16:50:15.018573Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Training for 50000 steps ...\nInterval 1 (0 steps performed)\n10000/10000 [==============================] - 112s 11ms/step - reward: -0.6304\n166 episodes - episode_reward: -37.855 [-60.000, 32.000] - loss: 1.326 - mae: 9.147 - mean_q: -10.958\n\nInterval 2 (10000 steps performed)\n 9343/10000 [===========================>..] - ETA: 7s - reward: -0.5558done, took 215.735 seconds\n","output_type":"stream"},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f5495b21bd0>"},"metadata":{}}]},{"cell_type":"code","source":"scores = dqn.test(env, nb_episodes=100, visualize=False)\nprint(np.mean(scores.history['episode_reward']))","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:50:20.124234Z","iopub.execute_input":"2022-10-06T16:50:20.124636Z","iopub.status.idle":"2022-10-06T16:50:26.427209Z","shell.execute_reply.started":"2022-10-06T16:50:20.124604Z","shell.execute_reply":"2022-10-06T16:50:26.425937Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Testing for 100 episodes ...\nEpisode 1: reward: 60.000, steps: 60\nEpisode 2: reward: 60.000, steps: 60\nEpisode 3: reward: 60.000, steps: 60\nEpisode 4: reward: 58.000, steps: 60\nEpisode 5: reward: 58.000, steps: 60\nEpisode 6: reward: 60.000, steps: 60\nEpisode 7: reward: 60.000, steps: 60\nEpisode 8: reward: 58.000, steps: 60\nEpisode 9: reward: 60.000, steps: 60\nEpisode 10: reward: 58.000, steps: 60\nEpisode 11: reward: 60.000, steps: 60\nEpisode 12: reward: 60.000, steps: 60\nEpisode 13: reward: 60.000, steps: 60\nEpisode 14: reward: 60.000, steps: 60\nEpisode 15: reward: 60.000, steps: 60\nEpisode 16: reward: 58.000, steps: 60\nEpisode 17: reward: 60.000, steps: 60\nEpisode 18: reward: 60.000, steps: 60\nEpisode 19: reward: 60.000, steps: 60\nEpisode 20: reward: 60.000, steps: 60\nEpisode 21: reward: 58.000, steps: 60\nEpisode 22: reward: 58.000, steps: 60\nEpisode 23: reward: 60.000, steps: 60\nEpisode 24: reward: 58.000, steps: 60\nEpisode 25: reward: 60.000, steps: 60\nEpisode 26: reward: 60.000, steps: 60\nEpisode 27: reward: 60.000, steps: 60\nEpisode 28: reward: 60.000, steps: 60\nEpisode 29: reward: 60.000, steps: 60\nEpisode 30: reward: 58.000, steps: 60\nEpisode 31: reward: 58.000, steps: 60\nEpisode 32: reward: 58.000, steps: 60\nEpisode 33: reward: 58.000, steps: 60\nEpisode 34: reward: 60.000, steps: 60\nEpisode 35: reward: 58.000, steps: 60\nEpisode 36: reward: 60.000, steps: 60\nEpisode 37: reward: 60.000, steps: 60\nEpisode 38: reward: 60.000, steps: 60\nEpisode 39: reward: 60.000, steps: 60\nEpisode 40: reward: 60.000, steps: 60\nEpisode 41: reward: 58.000, steps: 60\nEpisode 42: reward: 60.000, steps: 60\nEpisode 43: reward: 60.000, steps: 60\nEpisode 44: reward: 58.000, steps: 60\nEpisode 45: reward: 60.000, steps: 60\nEpisode 46: reward: 60.000, steps: 60\nEpisode 47: reward: 60.000, steps: 60\nEpisode 48: reward: 60.000, steps: 60\nEpisode 49: reward: 58.000, steps: 60\nEpisode 50: reward: 58.000, steps: 60\nEpisode 51: reward: 60.000, steps: 60\nEpisode 52: reward: 60.000, steps: 60\nEpisode 53: reward: 60.000, steps: 60\nEpisode 54: reward: 60.000, steps: 60\nEpisode 55: reward: 58.000, steps: 60\nEpisode 56: reward: 60.000, steps: 60\nEpisode 57: reward: 60.000, steps: 60\nEpisode 58: reward: 58.000, steps: 60\nEpisode 59: reward: 60.000, steps: 60\nEpisode 60: reward: 60.000, steps: 60\nEpisode 61: reward: 60.000, steps: 60\nEpisode 62: reward: 60.000, steps: 60\nEpisode 63: reward: 60.000, steps: 60\nEpisode 64: reward: 60.000, steps: 60\nEpisode 65: reward: 60.000, steps: 60\nEpisode 66: reward: 60.000, steps: 60\nEpisode 67: reward: 60.000, steps: 60\nEpisode 68: reward: 58.000, steps: 60\nEpisode 69: reward: 60.000, steps: 60\nEpisode 70: reward: 60.000, steps: 60\nEpisode 71: reward: 60.000, steps: 60\nEpisode 72: reward: 60.000, steps: 60\nEpisode 73: reward: 60.000, steps: 60\nEpisode 74: reward: 58.000, steps: 60\nEpisode 75: reward: 58.000, steps: 60\nEpisode 76: reward: 58.000, steps: 60\nEpisode 77: reward: 58.000, steps: 60\nEpisode 78: reward: 60.000, steps: 60\nEpisode 79: reward: 58.000, steps: 60\nEpisode 80: reward: 60.000, steps: 60\nEpisode 81: reward: 58.000, steps: 60\nEpisode 82: reward: 60.000, steps: 60\nEpisode 83: reward: 60.000, steps: 60\nEpisode 84: reward: 60.000, steps: 60\nEpisode 85: reward: 60.000, steps: 60\nEpisode 86: reward: 60.000, steps: 60\nEpisode 87: reward: 58.000, steps: 60\nEpisode 88: reward: 60.000, steps: 60\nEpisode 89: reward: 58.000, steps: 60\nEpisode 90: reward: 60.000, steps: 60\nEpisode 91: reward: 60.000, steps: 60\nEpisode 92: reward: 58.000, steps: 60\nEpisode 93: reward: 60.000, steps: 60\nEpisode 94: reward: 58.000, steps: 60\nEpisode 95: reward: 58.000, steps: 60\nEpisode 96: reward: 60.000, steps: 60\nEpisode 97: reward: 60.000, steps: 60\nEpisode 98: reward: 60.000, steps: 60\nEpisode 99: reward: 60.000, steps: 60\nEpisode 100: reward: 58.000, steps: 60\n59.36\n","output_type":"stream"}]},{"cell_type":"code","source":"_ = dqn.test(env, nb_episodes=15, visualize=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:50:29.822533Z","iopub.execute_input":"2022-10-06T16:50:29.823850Z","iopub.status.idle":"2022-10-06T16:50:30.831296Z","shell.execute_reply.started":"2022-10-06T16:50:29.823808Z","shell.execute_reply":"2022-10-06T16:50:30.829873Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Testing for 15 episodes ...\nEpisode 1: reward: 60.000, steps: 60\nEpisode 2: reward: 60.000, steps: 60\nEpisode 3: reward: 60.000, steps: 60\nEpisode 4: reward: 60.000, steps: 60\nEpisode 5: reward: 60.000, steps: 60\nEpisode 6: reward: 58.000, steps: 60\nEpisode 7: reward: 60.000, steps: 60\nEpisode 8: reward: 60.000, steps: 60\nEpisode 9: reward: 60.000, steps: 60\nEpisode 10: reward: 60.000, steps: 60\nEpisode 11: reward: 60.000, steps: 60\nEpisode 12: reward: 58.000, steps: 60\nEpisode 13: reward: 60.000, steps: 60\nEpisode 14: reward: 58.000, steps: 60\nEpisode 15: reward: 60.000, steps: 60\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Reloading Agent from Memory","metadata":{}},{"cell_type":"code","source":"dqn.save_weights('dqn_weights.h5f', overwrite=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:50:37.366448Z","iopub.execute_input":"2022-10-06T16:50:37.368074Z","iopub.status.idle":"2022-10-06T16:50:37.546829Z","shell.execute_reply.started":"2022-10-06T16:50:37.368010Z","shell.execute_reply":"2022-10-06T16:50:37.545261Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"del model\ndel dqn\ndel env","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:50:40.759801Z","iopub.execute_input":"2022-10-06T16:50:40.760418Z","iopub.status.idle":"2022-10-06T16:50:40.777501Z","shell.execute_reply.started":"2022-10-06T16:50:40.760360Z","shell.execute_reply":"2022-10-06T16:50:40.776437Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# env = gym.make('CartPole-v0')\nenv = ShowerEnv()\nactions = env.action_space.n\nstates = env.observation_space.shape\nmodel = build_model(states, actions)\ndqn = build_agent(model, actions)\ndqn.compile(adam_v2.Adam(learning_rate=1e-3), metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:50:43.788339Z","iopub.execute_input":"2022-10-06T16:50:43.788757Z","iopub.status.idle":"2022-10-06T16:50:44.618544Z","shell.execute_reply.started":"2022-10-06T16:50:43.788725Z","shell.execute_reply":"2022-10-06T16:50:44.617352Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"dqn.load_weights('dqn_weights.h5f')","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:50:48.133230Z","iopub.execute_input":"2022-10-06T16:50:48.133732Z","iopub.status.idle":"2022-10-06T16:50:48.503776Z","shell.execute_reply.started":"2022-10-06T16:50:48.133693Z","shell.execute_reply":"2022-10-06T16:50:48.502571Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"_ = dqn.test(env, nb_episodes=5, visualize=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T16:50:50.662368Z","iopub.execute_input":"2022-10-06T16:50:50.662854Z","iopub.status.idle":"2022-10-06T16:50:51.136011Z","shell.execute_reply.started":"2022-10-06T16:50:50.662817Z","shell.execute_reply":"2022-10-06T16:50:51.134806Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Testing for 5 episodes ...\nEpisode 1: reward: 58.000, steps: 60\nEpisode 2: reward: 58.000, steps: 60\nEpisode 3: reward: 58.000, steps: 60\nEpisode 4: reward: 60.000, steps: 60\nEpisode 5: reward: 58.000, steps: 60\n","output_type":"stream"}]}]}